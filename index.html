<!DOCTYPE html>
<html lang="en">

<head>
    <title>Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,400i|PT+Serif:700" rel="stylesheet">
    <link rel="stylesheet" href="dist/css/style.css">
    <link rel="stylesheet" href="dist/css/new_style.css">
    <!-- <link rel="stylesheet" href="css/index.css" /> -->
</head>

<body class="is-boxed has-animations">
    <div class="body-wrap boxed-container">
        <div class="header" id="header">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <h1>Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation </h1>

                <a href="https://sonia-raychaudhuri.github.io" target="_blank">Sonia Raychaudhuri<sup>1,2</sup></a>
                <a href="" target="_blank">Duy Ta<sup>1</sup></a>
                <a href="" target="_blank">Katrina Ashton<sup>1,3</sup></a>
                <a href="" target="_blank">Angel X. Chang<sup>2</sup></a>
                <a href="" target="_blank"">Jiuguang Wang<sup>1</sup></a>
                <a href="" target="_blank"">Bernadette Bucher<sup>1,4</sup></a>
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <span><sup>1</sup>Boston Dynamics AI Institute, &nbsp; <sup>2</sup>Simon Fraser University, &nbsp;
                    <sup>3</sup>University of Pennsylvania,
                    &nbsp; <sup>4</sup>University of Michigan 
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <a href="" target="_blank">under submission</a>
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <a href="https://arxiv.org/abs/2411.07848" target="_blank">arxiv</a> | 
                <a href="https://youtu.be/O26NiiL2XL4" target="_blank">Video</a> | 
                <a href="https://drive.google.com/file/d/1uA482B9U1rW2UYzP39qrHavZehbjvCwO/view?usp=sharing" target="_blank">OC-VLN Data</a> | 
                Code (coming soon)
            </div>

        </div>

        <main>
            <div class="container">
                <div class="row">
                    <div class="has-top-divider">&nbsp;</div>
                    <div class="col-lg-2 col-md-2 col-sm-2 col-xs-2"></div>
                    <div class="col-lg-10 col-md-10 col-sm-10 col-xs-10">
                        
                    </div>
                </div>
                <div class="row">
                    <div class="has-top-divider">&nbsp;</div>
                    <div class="text-center">
                        <image src="images/teaser.png" class="img-fluid" />
                    </div>
                    <br/>
                    <p>The robot is tasked with following a language instruction in an unseen environment to reach the final target (left). We
introduce Language-Inferred Factor Graph for Instruction Following (LIFGIF) (right), a novel method that infers a graph
from the instruction using an LLM, offline, and thereafter optimizes the graph with actual observations at every step, online,
during navigation. The robot selects the next waypoint from the inferred graph and moves toward it using a local planner.</p>
                </div>

                <div class="row">
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <h2 style="text-align: center;">Abstract</h2>
                        <p>
                            Large scale scenes such as multifloor homes can be robustly and efficiently mapped with a 3D graph of land- marks estimated jointly with robot poses in a factor graph, a technique commonly used in commercial robots such as drones and robot vacuums. In this work, we propose Language- Inferred Factor Graph for Instruction Following (LIFGIF), a zero-shot method to ground natural language instructions in such a map. LIFGIF also includes a policy for following natural language navigation instructions in a novel environ- ment while the map is constructed, enabling robust navigation performance in the physical world. To evaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in order to evaluate grounding of object-centric natural language navigation instructions. We compare to two state-of-the-art zero-shot baselines from related tasks, Object Goal Navigation and Vision Language Navigation, to demonstrate that LIFGIF outperforms them across all our evaluation metrics on OC- VLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for performing zero-shot object-centric instruction following in the real world on a Boston Dynamics Spot robot.
                        </p>
                    </div>

                </div>

                <div class="row">
                    <h2 style="text-align: center;">Method</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <p>
                            Our LIFGIF method stems from a key insight: language
instructions not only guide the robot’s navigation but also
encode crucial spatial information about the environment’s
layout. Even before making any observations, these instructions provide the robot with a preliminary understanding of
the environment’s map, albeit with significant uncertainty.
For instance, the instruction “move forward until you see
a chair.” implies the presence of a chair somewhere along
the forward direction (x-axis) relative to the robot’s current
position, even if we don’t know the exact distance to the
chair. By representing this spatial information as a factor
graph, we can integrate it as a prior into a traditional
factor-graph-based SLAM system. As the robot observes
landmarks mentioned in the instructions, the uncertainty
associated with these landmarks diminishes substantially,
thus helping the robot in localizing itself within the context
of the instructions during navigation. This effectively bridges
the gap between linguistic guidance and spatial awareness.
                        </p>
                        <br/>
                        <div class="col-lg-8 offset-lg-2 col-md-8 offset-md-2 col-sm-12 col-xs-12 text-center">
                            <image src="images/inferred_graph_example.png" class="img-fluid" />
                        </div>
                    </div>

                </div>

                <div class="row">
                    <h2 style="text-align: center;">Robot demonstrations on a spot</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                        <video controls="controls" width="800" height="600" name="Video Name">
                          <source src="images/real_video1.mov">
                        </video>
                        <br>
                        <strong>"Move forward to the bicycle. Turn right, then move to the chair. Turn left, and stop near the potted plant."</strong>
                    </div>

                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                        <video controls="controls" width="800" height="600" name="Video Name">
                          <source src="images/real_video2.mov">
                        </video>
                        <br>
                        <strong>"Move to the bicycle, turn left. move forward and stop at the tv."</strong>
                    </div>

                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                        <video controls="controls" width="800" height="600" name="Video Name">
                          <source src="images/real_video3.mov">
                        </video>
                        <br>
                        <strong>"Move forward to the chair. Turn right and move towards the bench. Stop there."</strong>
                    </div>
                </div>

                <div class="row">
                    <h2 style="text-align: center;">Evaluating in simulation</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                        <video controls="controls" width="800" height="600" name="Video Name">
                          <source src="images/sim_video.mov">
                        </video>
                        <br>
                        <strong>"Move forward to the brown couch. Stop at the fireplace."</strong>
                    </div>
                </div>

                <div class="row">&nbsp;&nbsp;</div>
                <div class="row">&nbsp;&nbsp;</div>
                
            </div>
        </main>
<!--         <div class="row">
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                        <h2>Citation</h2>
                        <pre xml:space="preserve" style="display: block;">
@misc{raychaudhuri2024nlslamocvlnnaturallanguage,
      title={NL-SLAM for OC-VLN: Natural Language Grounded SLAM for Object-Centric VLN}, 
      author={Sonia Raychaudhuri and Duy Ta and Katrina Ashton and Angel X. Chang and Jiuguang Wang and Bernadette Bucher},
      year={2024},
      eprint={2411.07848},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2411.07848}, 
} 
</pre>
                    </div>
                </div> -->
    </div>
    <!-- Bootstrap core JavaScript
        ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

</body>

</html>
